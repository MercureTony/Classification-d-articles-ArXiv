{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv #(on doit le faire car les fichiers csv doivent avoir le même nombre de colonnes par ligne, mais les abstracts couvrent plusieurs lignes)\n",
    "import re\n",
    "import nltk \n",
    "import math\n",
    "\n",
    "STOPWORDS = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\n",
    "             \"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\n",
    "             \"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "             \"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\n",
    "             \"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\n",
    "             \"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\n",
    "             \"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\n",
    "             \"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\n",
    "             \"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\n",
    "             \"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\n",
    "             \"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\n",
    "             \"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\n",
    "             \"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\n",
    "             \"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\n",
    "             \"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\n",
    "             \"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\n",
    "             \"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\n",
    "             \"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\n",
    "             \"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\n",
    "             \"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\n",
    "             \"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\n",
    "             \"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\n",
    "             \"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\n",
    "             \"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\n",
    "             \"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\n",
    "             \"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\n",
    "             \"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\n",
    "             \"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\n",
    "             \"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\n",
    "             \"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\n",
    "             \"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\n",
    "             \"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\n",
    "             \"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\n",
    "             \"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\n",
    "             \"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\n",
    "             \"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\n",
    "             \"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\n",
    "             \"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\n",
    "             \"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\n",
    "             \"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\n",
    "             \"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\n",
    "             \"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\n",
    "             \"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\n",
    "             \"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\n",
    "             \"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\n",
    "             \"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\n",
    "             \"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\n",
    "             \"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\n",
    "             \"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\n",
    "             \"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\n",
    "             \"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\n",
    "             \"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\n",
    "             \"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\n",
    "             \"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\n",
    "             \"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\n",
    "             \"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "#Importer nos données \n",
    "\n",
    "data = open('train.csv')\n",
    "csv_file = csv.reader(data)\n",
    "data_points = []\n",
    "\n",
    "for row in csv_file:\n",
    "    data_points.append(row)\n",
    "    \n",
    "data_points_array = np.array(data_points)\n",
    "\n",
    "data_without_punctuation = re.sub('\\W+',' ', data_points_array[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO: 7500 données\n",
    "\n",
    "#on sépare les abstract et les catégories du csv\n",
    "\n",
    "abstract_data = data_points_array[1:, 1] #\n",
    "\n",
    "category_data = data_points_array[1:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclasse = np.unique(category_data)\\nprint(classe)\\n\\ncounts = []\\n\\n#On crée une liste du nombre d'occurrences des classes\\nfor x in range(len(classe)):\\n    counts.append(0)\\n\\n#On compte le nombre de classes dans nos données\\nwhere = []\\nfor x in range(len(category_data)):\\n    where = np.where(classe ==category_data[x])\\n    counts[where[0][0]] += 1\\nprint(counts)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On commpte le nombre d'instance de chaque classe\n",
    "\"\"\"\n",
    "classe = np.unique(category_data)\n",
    "print(classe)\n",
    "\n",
    "counts = []\n",
    "\n",
    "#On crée une liste du nombre d'occurrences des classes\n",
    "for x in range(len(classe)):\n",
    "    counts.append(0)\n",
    "\n",
    "#On compte le nombre de classes dans nos données\n",
    "where = []\n",
    "for x in range(len(category_data)):\n",
    "    where = np.where(classe ==category_data[x])\n",
    "    counts[where[0][0]] += 1\n",
    "print(counts)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probabilité d'avoir une classe X dans nos données, prior P(c)\n",
    "\"\"\"\n",
    "prior = []\n",
    "\n",
    "for x in counts:\n",
    "    prior.append(x/len(category_data))\n",
    "print(prior)\n",
    "\"\"\"\n",
    "def count_priors(labels):\n",
    "    total = len(labels)\n",
    "    unique_classes = np.unique(labels)\n",
    "    count_classes = np.zeros(len(unique_classes))\n",
    "    for i, unique_class in enumerate(unique_classes):\n",
    "        count_classes[i] = np.count_nonzero(labels == unique_class)\n",
    "    \n",
    "    priors = count_classes/total\n",
    "    \n",
    "    priors_dict = dict(zip(unique_classes, priors))\n",
    "    return priors_dict    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compter le nombre de mot de chaque classe\n",
    "\n",
    "\n",
    "def extraction(phrase):\n",
    "    extract = []\n",
    "    phrase = re.sub('\\W+',' ', phrase)\n",
    "    phrase_array = phrase.splitlines()\n",
    "    for line in phrase_array:\n",
    "        word_array = line.split(' ')\n",
    "        for m in word_array:\n",
    "            m = m.lower().strip()\n",
    "            if m.lower() not in STOPWORDS and m != '':\n",
    "                extract.append(m)\n",
    "            \n",
    "    return extract #retourne une list de mots\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    entire = []\n",
    "    for abstract in corpus:\n",
    "        words = extraction(abstract)\n",
    "        entire = entire + words\n",
    "        \n",
    "    vocabulaire = np.unique(entire)\n",
    "    return vocabulaire\n",
    "\n",
    "\n",
    "#Nombre d'occurences total pour tous les mots selon une classe\n",
    "def sac_de_mot_general(sac, labels ,classe, vocabulary_bag):\n",
    "    size_vocab = len(vocabulary_bag)\n",
    "    mots_classe = []\n",
    "    bag = dict()\n",
    "    for i in range(len(sac)):\n",
    "        if classe == labels[i]:\n",
    "            mots = extraction(sac[i])\n",
    "            for mot in mots:\n",
    "                mots_classe.append(mot)\n",
    "                if mot not in bag.keys():\n",
    "                    bag[mot] = 1\n",
    "                else:\n",
    "                    bag[mot] += 1\n",
    "    \n",
    "    vocabulary_classe = np.unique(mots_classe)\n",
    "    \n",
    "    total = size_vocab #on ajoute la taille du vocabulaire\n",
    "    \n",
    "    words_not_in_class = np.setdiff1d(vocabulary_bag, vocabulary_classe)\n",
    "    \n",
    "    for i in bag:\n",
    "        total = total+ bag[i]\n",
    "    \n",
    "    \n",
    "    for i in bag:\n",
    "        bag[i] = bag[i]/total\n",
    "    \n",
    "    for i in words_not_in_class:\n",
    "        bag[i] = 1/total\n",
    "    \n",
    "    return bag\n",
    "\n",
    "\n",
    "def sac_de_mot_final(sac, labels):\n",
    "    dictio_final = {}\n",
    "    classe = np.unique(labels)\n",
    "    vocabulary_bag = get_vocab(sac)\n",
    "    for i in classe:\n",
    "        dictio_final[i] = sac_de_mot_general(sac, labels ,i, vocabulary_bag)\n",
    "        \n",
    "    return dictio_final\n",
    "        \n",
    "#Nombre d'occurences total pour un article\n",
    "def sac_de_mot(sac):\n",
    "    bag = dict()\n",
    "    mots = extraction(sac)\n",
    "    for mot in mots:\n",
    "        if mot not in bag.keys():\n",
    "            bag[mot] = 1\n",
    "        else:\n",
    "            bag[mot] += 1\n",
    "    return bag\n",
    "\n",
    "#Probabillité d'avoir un mot sachant une classe, vraisemblance P(c/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifieurBayesienNaif:\n",
    "    def train(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.probabilities = sac_de_mot_final(data, labels)\n",
    "        self.priors = count_priors(labels)\n",
    "        \n",
    "    def classify(self, test_input):\n",
    "        shape_of_test = test_input.shape\n",
    "        size = shape_of_test[0]\n",
    "        pred = []\n",
    "        if size == 1:\n",
    "            input_dictio = sac_de_mot(test_input)\n",
    "            likelihood_class = {}\n",
    "            for bag_proba in self.probabilities:\n",
    "                prior = self.priors[bag_proba]\n",
    "                probability = prior  #on commence avec la valeur du prior vu que l'on va le multipier au produit\n",
    "                for key in input_dictio:\n",
    "                    number_of_occurence = input_dictio[key]\n",
    "                    if key in self.probabilities[bag_proba]:\n",
    "                        proba_word_given_class = self.probabilities[bag_proba][key]\n",
    "                        proba = proba_word_given_class ** number_of_occurence\n",
    "                    else:\n",
    "                        proba = 1\n",
    "                    probability = probability * proba\n",
    "                    \n",
    "                likelihood_class[bag_proba] = probability\n",
    "                \n",
    "            values_prob = list(likelihood_class.values())\n",
    "            keys_prob = list(likelihood_class.keys())\n",
    "            best_probability = max(values_prob)\n",
    "            class_predicted = keys_prob[values_prob.index(best_probability)]\n",
    "            pred.append(class_predicted)\n",
    "                \n",
    "                    \n",
    "        else:\n",
    "            for i, input_data_row in enumerate(test_input):\n",
    "                input_dictio = sac_de_mot(input_data_row)\n",
    "                likelihood_class = {}\n",
    "                for bag_proba in self.probabilities:\n",
    "                    prior = self.priors[bag_proba]\n",
    "                    probability = math.log(prior) #on commence avec la valeur du prior vu que l'on va le multipier au produit\n",
    "                    for key in input_dictio:\n",
    "                        number_of_occurence = input_dictio[key]\n",
    "                        if key in self.probabilities[bag_proba]:\n",
    "                            proba_word_given_class = self.probabilities[bag_proba][key]\n",
    "                            proba = proba_word_given_class ** number_of_occurence\n",
    "                            log_proba = math.log(proba)\n",
    "                            \n",
    "                        else:\n",
    "                            proba = 1\n",
    "                            log_proba = 0\n",
    "                        probability = probability + log_proba\n",
    "                        \n",
    "                    likelihood_class[bag_proba] = probability\n",
    "                    \n",
    "                values_prob = list(likelihood_class.values())\n",
    "                keys_prob = list(likelihood_class.keys())\n",
    "                best_probability = max(values_prob)\n",
    "                class_predicted = keys_prob[values_prob.index(best_probability)]\n",
    "                pred.append(class_predicted)\n",
    "                \n",
    "        return pred\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open('test.csv')\n",
    "csv_file_test = csv.reader(test)\n",
    "test_points = []\n",
    "\n",
    "for row in csv_file_test:\n",
    "    test_points.append(row)\n",
    "    \n",
    "test_points_array = np.array(test_points)\n",
    "\n",
    "test_abstract_data = test_points_array[1:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ClassifieurBayesienNaif()\n",
    "classifier.train(abstract_data,category_data)\n",
    "classes = classifier.classify(test_abstract_data)\n",
    "classes_array = np.array(classes)\n",
    "indices = np.arange(15000)\n",
    "final = np.column_stack((indices,classes_array))\n",
    "header = [\"Id\", \"Category\"]\n",
    "to_csv = np.vstack([header, final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Id' 'Category']\n",
      " ['0' 'stat.ML']\n",
      " ['1' 'astro-ph.SR']\n",
      " ['2' 'astro-ph.SR']\n",
      " ['3' 'math.AP']\n",
      " ['4' 'cs.LG']\n",
      " ['5' 'gr-qc']\n",
      " ['6' 'math.CO']\n",
      " ['7' 'cond-mat.mtrl-sci']\n",
      " ['8' 'astro-ph.CO']]\n"
     ]
    }
   ],
   "source": [
    "print(to_csv[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('submission.csv', to_csv, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
