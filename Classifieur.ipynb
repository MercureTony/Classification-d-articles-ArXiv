{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The energy released in a solar flare is partitioned between thermal and\n",
      "non-thermal particle energy and lost to thermal conduction and radiation over a\n",
      "broad range of wavelengths. It is difficult to determine the conductive losses\n",
      "and the energy radiated at transition region temperatures during the impulsive\n",
      "phases of flares. We use UVCS measurements of O VI photons produced by 5 flares\n",
      "and subsequently scattered by O VI ions in the corona to determine the 5.0 <\n",
      "log T < 6.0 transition region luminosities. We compare them with the rates of\n",
      "increase of thermal energy and the conductive losses deduced from RHESSI and\n",
      "GOES X-ray data using areas from RHESSI images to estimate the loop volumes,\n",
      "cross-sectional areas and scale lengths. The transition region luminosities\n",
      "during the impulsive phase exceed the X-ray luminosities for the first few\n",
      "minutes, but they are smaller than the rates of increase of thermal energy\n",
      "unless the filling factor of the X-ray emitting gas is ~ 0.01. The estimated\n",
      "conductive losses from the hot gas are too large to be balanced by radiative\n",
      "losses or heating of evaporated plasma, and we conclude that the area of the\n",
      "flare magnetic flux tubes is much smaller than the effective area measured by\n",
      "RHESSI during this phase of the flares. For the 2002 July 23 flare, the energy\n",
      "deposited by non-thermal particles exceeds the X-ray and UV energy losses and\n",
      "the rate of increase of the thermal energy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv #(on doit le faire car les fichiers csv doivent avoir le même nombre de colonnes par ligne, mais les abstracts couvrent plusieurs lignes)\n",
    "import re\n",
    "import nltk \n",
    "\n",
    "STOPWORDS = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\n",
    "             \"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\n",
    "             \"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\n",
    "             \"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\n",
    "             \"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\n",
    "             \"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\n",
    "             \"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\n",
    "             \"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\n",
    "             \"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\n",
    "             \"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\n",
    "             \"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\n",
    "             \"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\n",
    "             \"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\n",
    "             \"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\n",
    "             \"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\n",
    "             \"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\n",
    "             \"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\n",
    "             \"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\n",
    "             \"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\n",
    "             \"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\n",
    "             \"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\n",
    "             \"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\n",
    "             \"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\n",
    "             \"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\n",
    "             \"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\n",
    "             \"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\n",
    "             \"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\n",
    "             \"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\n",
    "             \"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\n",
    "             \"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\n",
    "             \"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\n",
    "             \"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\n",
    "             \"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\n",
    "             \"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\n",
    "             \"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\n",
    "             \"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\n",
    "             \"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\n",
    "             \"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\n",
    "             \"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\n",
    "             \"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\n",
    "             \"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\n",
    "             \"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\n",
    "             \"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\n",
    "             \"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\n",
    "             \"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\n",
    "             \"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\n",
    "             \"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\n",
    "             \"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\n",
    "             \"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\n",
    "             \"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\n",
    "             \"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\n",
    "             \"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\n",
    "             \"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\n",
    "             \"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\n",
    "             \"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\n",
    "             \"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\n",
    "#Importer nos données \n",
    "\n",
    "train = open('train.csv')\n",
    "csv_file = csv.reader(train)\n",
    "data_points = []\n",
    "\n",
    "for row in csv_file:\n",
    "    data_points.append(row)\n",
    "\n",
    "df = np.array(data_points)\n",
    "\n",
    "print(data_points_array[1][1])\n",
    "\n",
    "# Fonction pour nettoyer les données, data=corpus, row=rangée, column=ordonnée)\n",
    "def dc(data):\n",
    "    data = data.lower()\n",
    "    data = data.strip()\n",
    "    \n",
    "    data = re.sub('\\n',' ',data)\n",
    "    data = re.sub(\"\\d+\", \"\",data)\n",
    "    data = re.sub('[^\\w\\s]','',data)\n",
    "    data = re.sub('\\[[^]]*\\]', '',data)\n",
    "    data = re.sub(\"<.*?>\", \"\",data)\n",
    "    data =  re.sub(\"\\s+\", \" \",data) \n",
    "    data = data.encode('ascii','ignore').decode()\n",
    "    return data\n",
    "\n",
    "#dc(df,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n"
     ]
    }
   ],
   "source": [
    "#INFO: 7500 données\n",
    "\n",
    "#on sépare les abstract et les catégories du csv\n",
    "\n",
    "abstract_data = data_points_array[1:, 1] #données de l'abstract\n",
    "\n",
    "category_data = data_points_array[1:, 2] #données des catégories\n",
    "\n",
    "print(len(abstract_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'astro-ph': 0.06666666666666667, 'astro-ph.CO': 0.06666666666666667, 'astro-ph.GA': 0.06666666666666667, 'astro-ph.SR': 0.06666666666666667, 'cond-mat.mes-hall': 0.06666666666666667, 'cond-mat.mtrl-sci': 0.06666666666666667, 'cs.LG': 0.06666666666666667, 'gr-qc': 0.06666666666666667, 'hep-ph': 0.06666666666666667, 'hep-th': 0.06666666666666667, 'math.AP': 0.06666666666666667, 'math.CO': 0.06666666666666667, 'physics.optics': 0.06666666666666667, 'quant-ph': 0.06666666666666667, 'stat.ML': 0.06666666666666667}\n"
     ]
    }
   ],
   "source": [
    "#Probabilité d'avoir une classe X dans nos données, prior P(c)\n",
    "def count_priors(labels):\n",
    "    total = len(labels)\n",
    "    unique_classes = np.unique(labels)\n",
    "    count_classes = np.zeros(len(unique_classes))\n",
    "    for i, unique_class in enumerate(unique_classes):\n",
    "        count_classes[i] = np.count_nonzero(labels == unique_class)\n",
    "    \n",
    "    priors = count_classes/total\n",
    "    \n",
    "    priors_dict = dict(zip(unique_classes, priors))\n",
    "    return priors_dict    \n",
    "        \n",
    "print(count_priors(category_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['energy', 'released', 'solar', 'flare', 'partitioned', 'thermal', 'nonthermal', 'particle', 'energy', 'lost', 'thermal', 'conduction', 'radiation', 'broad', 'range', 'wavelengths', 'difficult', 'determine', 'conductive', 'losses', 'energy', 'radiated', 'transition', 'region', 'temperatures', 'impulsive', 'phases', 'flares', 'uvcs', 'measurements', 'vi', 'photons', 'produced', 'flares', 'subsequently', 'scattered', 'vi', 'ions', 'corona', 'determine', 'log', 'transition', 'region', 'luminosities', 'compare', 'rates', 'increase', 'thermal', 'energy', 'conductive', 'losses', 'deduced', 'rhessi', 'xray', 'data', 'areas', 'rhessi', 'images', 'estimate', 'loop', 'volumes', 'crosssectional', 'areas', 'scale', 'lengths', 'transition', 'region', 'luminosities', 'impulsive', 'phase', 'exceed', 'xray', 'luminosities', 'minutes', 'smaller', 'rates', 'increase', 'thermal', 'energy', 'filling', 'factor', 'xray', 'emitting', 'gas', 'estimated', 'conductive', 'losses', 'hot', 'gas', 'large', 'balanced', 'radiative', 'losses', 'heating', 'evaporated', 'plasma', 'conclude', 'area', 'flare', 'magnetic', 'flux', 'tubes', 'smaller', 'effective', 'area', 'measured', 'rhessi', 'phase', 'flares', 'july', 'flare', 'energy', 'deposited', 'nonthermal', 'particles', 'exceeds', 'xray', 'uv', 'energy', 'losses', 'rate', 'increase', 'thermal', 'energy']\n"
     ]
    }
   ],
   "source": [
    "#Compter le nombre de mot de chaque classe\n",
    "def extraction(phrase): #prend en paramètre un article et renvoie les mots\n",
    "    extract = []\n",
    "    phrase = dc(phrase)\n",
    "    phrase_array = phrase.splitlines()\n",
    "    for line in phrase_array:\n",
    "        word_array = line.split(' ')\n",
    "        for m in word_array:\n",
    "            m = m.lower().strip()\n",
    "            if m.lower() not in STOPWORDS and m != '':\n",
    "                extract.append(m)\n",
    "            \n",
    "    return extract #retourne une list de mots\n",
    "\n",
    "print(extraction(abstract_data[0]))\n",
    "\n",
    "def get_vocab(corpus):\n",
    "    entire = []\n",
    "    for abstract in corpus:\n",
    "        words = extraction(abstract)\n",
    "        entire = entire + words\n",
    "        \n",
    "    vocabulaire = np.unique(entire)\n",
    "    return vocabulaire\n",
    "\n",
    "\n",
    "#Nombre d'occurences total pour tous les mots selon une classe\n",
    "def sac_de_mot_general(sac, labels ,classe, vocabulary_bag):\n",
    "    size_vocab = len(vocabulary_bag)\n",
    "    mots_classe = []\n",
    "    bag = dict()\n",
    "    for i in range(len(sac)):\n",
    "        if classe == labels[i]:\n",
    "            mots = extraction(sac[i])\n",
    "            for mot in mots:\n",
    "                mots_classe.append(mot)\n",
    "                if mot not in bag.keys():\n",
    "                    bag[mot] = 1\n",
    "                else:\n",
    "                    bag[mot] += 1\n",
    "    \n",
    "    vocabulary_classe = np.unique(mots_classe)\n",
    "    \n",
    "    total = size_vocab #on ajoute la taille du vocabulaire\n",
    "    \n",
    "    words_not_in_class = np.setdiff1d(vocabulary_bag, vocabulary_classe)\n",
    "    \n",
    "    for i in bag:\n",
    "        total = total+ bag[i]\n",
    "    \n",
    "    \n",
    "    for i in bag:\n",
    "        bag[i] = bag[i]/total\n",
    "    \n",
    "    for i in words_not_in_class:\n",
    "        bag[i] = 1/total\n",
    "    \n",
    "    return bag\n",
    "\n",
    "\n",
    "def sac_de_mot_final(sac, labels):\n",
    "    dictio_final = {}\n",
    "    classe = np.unique(labels)\n",
    "    vocabulary_bag = get_vocab(sac)\n",
    "    for i in classe:\n",
    "        dictio_final[i] = sac_de_mot_general(sac, labels ,i, vocabulary_bag)\n",
    "        \n",
    "    return dictio_final\n",
    "        \n",
    "#Nombre d'occurences total pour un article\n",
    "def sac_de_mot(sac):\n",
    "    bag = dict()\n",
    "    mots = extraction(sac)\n",
    "    for mot in mots:\n",
    "        if mot not in bag.keys():\n",
    "            bag[mot] = 1\n",
    "        else:\n",
    "            bag[mot] += 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifieurBayesienNaif:\n",
    "    def train(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.probabilities = sac_de_mot_final(data, labels)\n",
    "        self.priors = count_priors(labels)\n",
    "        \n",
    "    def classify(self, test_input):\n",
    "        shape_of_test = test_input.shape\n",
    "        size = shape_of_test[0]\n",
    "        pred = []\n",
    "        if size == 1:\n",
    "            input_dictio = sac_de_mot(test_input)\n",
    "            likelihood_class = {}\n",
    "            for bag_proba in self.probabilities:\n",
    "                prior = self.priors[bag_proba]\n",
    "                probability = prior  #on commence avec la valeur du prior vu que l'on va le multipier au produit\n",
    "                for key in input_dictio:\n",
    "                    number_of_occurence = input_dictio[key]\n",
    "                    if key in self.probabilities[bag_proba]:\n",
    "                        proba_word_given_class = self.probabilities[bag_proba][key]\n",
    "                        proba = proba_word_given_class ** number_of_occurence\n",
    "                    else:\n",
    "                        print(key + \" is not a word in initial vocabulary\")\n",
    "                        proba = 1\n",
    "                    probability = probability * proba\n",
    "                    \n",
    "                likelihood_class[bag_proba] = probability\n",
    "                \n",
    "            values_prob = list(likelihood_class.values())\n",
    "            keys_prob = list(likelihood_class.keys())\n",
    "            best_probability = max(values_prob)\n",
    "            class_predicted = keys_prob[values_prob.index(best_probability)]\n",
    "            pred.append(class_predicted)\n",
    "                \n",
    "                    \n",
    "        else:\n",
    "            for i, input_data_row in enumerate(test_input):\n",
    "                input_dictio = sac_de_mot(input_data_row)\n",
    "                likelihood_class = {}\n",
    "                for bag_proba in self.probabilities:\n",
    "                    prior = self.priors[bag_proba]\n",
    "                    probability = prior #on commence avec la valeur du prior vu que l'on va le multipier au produit\n",
    "                    for key in input_dictio:\n",
    "                        number_of_occurence = input_dictio[key]\n",
    "                        if key in self.probabilities[bag_proba]:\n",
    "                            proba_word_given_class = self.probabilities[bag_proba][key]\n",
    "                            proba = proba_word_given_class ** number_of_occurence\n",
    "                            \n",
    "                        else:\n",
    "                            print(key + \" is not a word in initial vocabulary\")\n",
    "                            proba = 1\n",
    "                        probability = probability * proba\n",
    "                        \n",
    "                    likelihood_class[bag_proba] = probability\n",
    "                    \n",
    "                values_prob = list(likelihood_class.values())\n",
    "                keys_prob = list(likelihood_class.keys())\n",
    "                best_probability = max(values_prob)\n",
    "                class_predicted = keys_prob[values_prob.index(best_probability)]\n",
    "                pred.append(class_predicted)\n",
    "                \n",
    "        return pred\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "test = open('test.csv')\n",
    "csv_file_test = csv.reader(test)\n",
    "test_points = []\n",
    "\n",
    "for row in csv_file_test:\n",
    "    test_points.append(row)\n",
    "    \n",
    "test_points_array = np.array(test_points)\n",
    "\n",
    "test_abstract_data = test_points_array[1:, 1]\n",
    "\n",
    "print(test_abstract_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "l_calculated is not a word in initial vocabulary\n",
      "maximumprojection is not a word in initial vocabulary\n",
      "l_maxprojection is not a word in initial vocabulary\n",
      "nuclearnorm is not a word in initial vocabulary\n",
      "l_subspace is not a word in initial vocabulary\n",
      "directionofarrival is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "collisionaltriple is not a word in initial vocabulary\n",
      "posits is not a word in initial vocabulary\n",
      "mildhierarchical is not a word in initial vocabulary\n",
      "gaiaresolved is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n",
      "omegawhite is not a word in initial vocabulary\n",
      "omegacam is not a word in initial vocabulary\n",
      "ugrihalpha is not a word in initial vocabulary\n",
      "altaz is not a word in initial vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stat.ML', 'astro-ph', 'astro-ph']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ClassifieurBayesienNaif()\n",
    "classifier.train(abstract_data,category_data)\n",
    "classifier.classify(test_abstract_data[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On commpte le nombre d'instance de chaque classe\n",
    "\"\"\"\n",
    "classe = np.unique(category_data)\n",
    "print(classe)\n",
    "\n",
    "counts = []\n",
    "\n",
    "#On crée une liste du nombre d'occurrences des classes\n",
    "for x in range(len(classe)):\n",
    "    counts.append(0)\n",
    "\n",
    "#On compte le nombre de classes dans nos données\n",
    "where = []\n",
    "for x in range(len(category_data)):\n",
    "    where = np.where(classe ==category_data[x])\n",
    "    counts[where[0][0]] += 1\n",
    "print(counts)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "prior = []\n",
    "\n",
    "for x in counts:\n",
    "    prior.append(x/len(category_data))\n",
    "print(prior)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
